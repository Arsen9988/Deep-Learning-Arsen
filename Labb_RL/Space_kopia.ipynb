{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras  # Keras är ett API för att bygga och träna neurala nätverk, som ofta används med TensorFlow som backend.\n",
    "from keras import layers  # layers används för att bygga de olika delarna (lager) i ett neuralt nätverk, t.ex. Dense- och Convolutional-lager.\n",
    "import gymnasium as gym  # Gymnasium är en plattform för att skapa miljöer för reinforcement learning (RL).\n",
    "from gymnasium.wrappers.frame_stack import FrameStack  \n",
    "# FrameStack är en wrapper från Gymnasium som kombinerar flera på varandra följande frames till en enda observation.\n",
    "# Detta är användbart i spel där kontext från tidigare frames behövs, exempelvis rörelseriktning i Atari-spel.\n",
    "\n",
    "from gymnasium.wrappers.atari_preprocessing import AtariPreprocessing  \n",
    "# AtariPreprocessing är en inbyggd funktion för att förbehandla bilder från Atari-spel.\n",
    "# Den konverterar bilder till gråskala, minskar deras storlek och beskär dem för att underlätta inlärning.\n",
    "\n",
    "import numpy as np  # NumPy används för numeriska operationer som matriser, slumpval och matematiska beräkningar.\n",
    "import tensorflow as tf  # TensorFlow används som backend för att implementera och träna neurala nätverk.\n",
    "import ale_py  # ale_py är en wrapper för Arcade Learning Environment (ALE) som tillhandahåller Atari-spel till Gymnasium.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Hyperparametrar för RL-träningen\n",
    "seed = 42  # Slumpfrö för att göra experimenten reproducerbara.\n",
    "gamma = 0.99  # Diskonteringsfaktor för framtida belöningar. Värden nära 1 prioriterar långsiktiga belöningar.\n",
    "epsilon = 1.0  # Startvärde för epsilon i epsilon-greedy-algoritmen, som används för att utforska.\n",
    "epsilon_min = 0.1  # Minsta värde för epsilon, vilket begränsar slumpmässiga val under utforskning.\n",
    "epsilon_max = 1.0  # Maxvärde för epsilon, används vid start.\n",
    "epsilon_interval = (epsilon_max - epsilon_min)  \n",
    "# Intervall för att gradvis minska epsilon under träning, från maximal utforskning till kontrollerad utforskning.\n",
    "\n",
    "batch_size = 32  # Antal exempel som används vid varje träningssteg.\n",
    "max_steps_per_episode = 10000  # Max antal steg i en episod innan den avslutas.\n",
    "max_episodes = 0  # Max antal episoder (0 innebär obegränsat).\n",
    "max_frames = 1e6  # Max antal bildramar som ska bearbetas innan träningen avslutas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# Registrerar Atari-miljöer via ALE.\n",
    "gym.register_envs(ale_py)\n",
    "# Skapar Atari Breakout-miljön.\n",
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"rgb_array\")  \n",
    "# Spelmiljö: Breakout utan att hoppa över några frames. render_mode=\"rgb_array\" gör att vi får bilddata.\n",
    "env = AtariPreprocessing(env)  \n",
    "\n",
    "# Applicerar förbehandling på miljön. Detta inkluderar:\n",
    "# - Omvandling till gråskala\n",
    "# - Bildskalning till 84x84 pixlar\n",
    "# - Normalisering av pixlar.\n",
    "\n",
    "env = FrameStack(env, 4)  \n",
    "# Staplar fyra på varandra följande frames för att ge modellen temporal kontext (rörelseinformation).\n",
    "\n",
    "# Triggerfunktion för att spela in video var 1000:e timsteg.\n",
    "trigger = lambda t: t % 500 == 0\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=\"videos\", episode_trigger=trigger, disable_logger=True)  \n",
    "# Wrapper för att spela in videos av episoder som uppfyller trigger-funktionen.\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "  # Antal möjliga handlingar i Breakout (t.ex. vänster, höger, stanna, skjut).\n",
    "  \n",
    "action_meanings = env.unwrapped.get_action_meanings()\n",
    "print(f\"{num_actions} {action_meanings}\") # 'NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_model():\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            layers.Conv2D(32, kernel_size=8, strides=4, activation=\"relu\"),  \n",
    "            # Första konvolutionslagret. \n",
    "            # - 32 filter, storlek 8x8, med steg 4.\n",
    "            # - Aktiveringsfunktion: ReLU för att introducera icke-linearitet.\n",
    "            \n",
    "            layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\"),  \n",
    "            # Andra konvolutionslagret.\n",
    "            # - 64 filter, storlek 4x4, med steg 2.\n",
    "            \n",
    "            layers.Conv2D(64, kernel_size=3, strides=1, activation=\"relu\"),  \n",
    "            # Tredje konvolutionslagret.\n",
    "            # - 64 filter, storlek 3x3, med steg 1.\n",
    "            \n",
    "            layers.Flatten(),  \n",
    "            # Flatten omvandlar 2D/3D-utgång till en 1D-vektor för att förbereda för Dense-lager.\n",
    "\n",
    "            layers.Dense(512, activation=\"relu\"),  \n",
    "            # Täckt lager med 512 neuroner. ReLU används för icke-linearitet.\n",
    "            \n",
    "            layers.Dense(num_actions, activation=\"linear\")  \n",
    "            # Utgångslager med antal neuroner som matchar antalet åtgärder (num_actions). \n",
    "            # Aktiveringsfunktion: linear eftersom vi approximerar Q-värden.\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(data):\n",
    "    return np.transpose(data, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skapa den primära modellen och en target-modell (för stabilitet vid uppdateringar).\n",
    "model = create_q_model()\n",
    "model_target = create_q_model()\n",
    "\n",
    "# Optimizer för träning: Adam med en lärhastighet på 0.00025 och gradientklippning för stabilitet.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Buffertar och andra variabler för att lagra träningens historia.\n",
    "action_history = []  # Historik av valda åtgärder.\n",
    "state_history = []  # Historik av observerade tillstånd.\n",
    "state_next_history = []  # Historik av efterföljande tillstånd.\n",
    "rewards_history = []  # Historik av erhållna belöningar.\n",
    "done_history = []  # Historik av episod-slutstatus.\n",
    "episode_reward_history = []  # Belöningar för varje avslutad episod.\n",
    "running_reward = 0  # Medelbelöning över senaste episoderna.\n",
    "episode_count = 0  # Antal episoder hittills.\n",
    "frame_count = 0  # Totalt antal processade frames.\n",
    "\n",
    "# Parametrar för epsilon-greedy policy.\n",
    "epsilon_random_frames = 50000  # Antal frames där endast slumpmässiga åtgärder används.\n",
    "epsilon_greedy_frames = 1000000.0  # Antal frames över vilka epsilon avtar från 1.0 till 0.1.\n",
    "\n",
    "# Maximal replay buffer-storlek och träningskonfiguration.\n",
    "max_memory_length = 1000000  # Max antal lagrade övergångar i replay-buffer.\n",
    "update_after_actions = 4  # Antal actions mellan varje träningsuppdatering.\n",
    "update_target_network = 10000  # Frekvens (i frames) för att uppdatera target-modellen.\n",
    "loss_function = keras.losses.Huber()  # Huberförlust, robust för utliggare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: 110.0\n",
      "Episode 1: 75.0\n",
      "Episode 2: 120.0\n",
      "Episode 3: 165.0\n",
      "Episode 4: 15.0\n",
      "Episode 5: 90.0\n",
      "Episode 6: 210.0\n",
      "Episode 7: 80.0\n",
      "Episode 8: 140.0\n",
      "Episode 9: 95.0\n",
      "Episode 10: 135.0\n",
      "Episode 11: 120.0\n",
      "Episode 12: 205.0\n",
      "Episode 13: 410.0\n",
      "Episode 14: 210.0\n",
      "Episode 15: 210.0\n",
      "Episode 16: 110.0\n",
      "Episode 17: 140.0\n",
      "Episode 18: 145.0\n",
      "Best score of last 100: 410.0, running reward: 146.58 at episode 19, frame count 10000\n",
      "Episode 19: 100.0\n",
      "Episode 20: 285.0\n",
      "Episode 21: 75.0\n",
      "Episode 22: 65.0\n",
      "Episode 23: 120.0\n",
      "Episode 24: 110.0\n",
      "Episode 25: 155.0\n",
      "Episode 26: 140.0\n",
      "Episode 27: 65.0\n",
      "Episode 28: 105.0\n",
      "Episode 29: 130.0\n",
      "Episode 30: 45.0\n",
      "Episode 31: 155.0\n",
      "Episode 32: 320.0\n",
      "Episode 33: 120.0\n",
      "Episode 34: 55.0\n",
      "Episode 35: 235.0\n",
      "Episode 36: 110.0\n",
      "Episode 37: 135.0\n",
      "Episode 38: 50.0\n",
      "Best score of last 100: 410.0, running reward: 137.44 at episode 39, frame count 20000\n",
      "Episode 39: 240.0\n",
      "Episode 40: 105.0\n",
      "Episode 41: 110.0\n",
      "Episode 42: 95.0\n",
      "Episode 43: 45.0\n",
      "Episode 44: 440.0\n",
      "Episode 45: 120.0\n",
      "Episode 46: 180.0\n",
      "Episode 47: 105.0\n",
      "Episode 48: 105.0\n",
      "Episode 49: 210.0\n",
      "Episode 50: 105.0\n",
      "Episode 51: 55.0\n",
      "Episode 52: 160.0\n",
      "Episode 53: 110.0\n",
      "Episode 54: 30.0\n",
      "Episode 55: 45.0\n",
      "Episode 56: 205.0\n",
      "Episode 57: 105.0\n",
      "Episode 58: 65.0\n",
      "Episode 59: 105.0\n",
      "Best score of last 100: 440.0, running reward: 135.00 at episode 60, frame count 30000\n",
      "Episode 60: 115.0\n",
      "Episode 61: 135.0\n",
      "Episode 62: 260.0\n",
      "Episode 63: 590.0\n",
      "Episode 64: 380.0\n",
      "Episode 65: 450.0\n",
      "Episode 66: 80.0\n",
      "Episode 67: 110.0\n",
      "Episode 68: 210.0\n",
      "Episode 69: 135.0\n",
      "Episode 70: 110.0\n",
      "Episode 71: 105.0\n",
      "Episode 72: 215.0\n",
      "Episode 73: 130.0\n",
      "Episode 74: 210.0\n",
      "Episode 75: 120.0\n",
      "Episode 76: 35.0\n",
      "Best score of last 100: 590.0, running reward: 149.22 at episode 77, frame count 40000\n",
      "Episode 77: 70.0\n",
      "Episode 78: 135.0\n",
      "Episode 79: 245.0\n",
      "Episode 80: 120.0\n",
      "Episode 81: 90.0\n",
      "Episode 82: 210.0\n",
      "Episode 83: 110.0\n",
      "Episode 84: 145.0\n",
      "Episode 85: 180.0\n",
      "Episode 86: 85.0\n",
      "Episode 87: 165.0\n",
      "Episode 88: 245.0\n",
      "Episode 89: 105.0\n",
      "Episode 90: 150.0\n",
      "Episode 91: 235.0\n",
      "Episode 92: 200.0\n",
      "Episode 93: 45.0\n",
      "Episode 94: 120.0\n",
      "Best score of last 100: 590.0, running reward: 148.89 at episode 95, frame count 50000\n",
      "Episode 95: 195.0\n",
      "Episode 96: 120.0\n",
      "Episode 97: 180.0\n",
      "Episode 98: 180.0\n",
      "Episode 99: 125.0\n",
      "Episode 100: 225.0\n",
      "Episode 101: 285.0\n",
      "Episode 102: 225.0\n",
      "Episode 103: 55.0\n",
      "Episode 104: 30.0\n",
      "Episode 105: 295.0\n",
      "Episode 106: 35.0\n",
      "Episode 107: 135.0\n",
      "Episode 108: 380.0\n",
      "Episode 109: 60.0\n",
      "Episode 110: 260.0\n",
      "Episode 111: 260.0\n",
      "Best score of last 100: 590.0, running reward: 158.35 at episode 112, frame count 60000\n",
      "Episode 112: 180.0\n",
      "Episode 113: 125.0\n",
      "Episode 114: 315.0\n",
      "Episode 115: 85.0\n",
      "Episode 116: 80.0\n",
      "Episode 117: 325.0\n",
      "Episode 118: 110.0\n",
      "Episode 119: 140.0\n",
      "Episode 120: 240.0\n",
      "Episode 121: 90.0\n",
      "Episode 122: 160.0\n",
      "Episode 123: 125.0\n",
      "Episode 124: 175.0\n",
      "Episode 125: 20.0\n",
      "Episode 126: 125.0\n",
      "Episode 127: 45.0\n",
      "Episode 128: 80.0\n",
      "Episode 129: 135.0\n",
      "Episode 130: 330.0\n",
      "Episode 131: 50.0\n",
      "Best score of last 100: 590.0, running reward: 157.90 at episode 132, frame count 70000\n",
      "Episode 132: 225.0\n",
      "Episode 133: 65.0\n",
      "Episode 134: 95.0\n",
      "Episode 135: 105.0\n",
      "Episode 136: 90.0\n",
      "Episode 137: 80.0\n",
      "Episode 138: 120.0\n",
      "Episode 139: 45.0\n",
      "Episode 140: 65.0\n",
      "Episode 141: 120.0\n",
      "Episode 142: 125.0\n",
      "Episode 143: 80.0\n",
      "Episode 144: 50.0\n",
      "Episode 145: 210.0\n",
      "Episode 146: 365.0\n",
      "Episode 147: 235.0\n",
      "Episode 148: 275.0\n",
      "Episode 149: 340.0\n",
      "Episode 150: 210.0\n",
      "Best score of last 100: 590.0, running reward: 158.05 at episode 151, frame count 80000\n",
      "Episode 151: 180.0\n",
      "Episode 152: 90.0\n",
      "Episode 153: 20.0\n",
      "Episode 154: 415.0\n",
      "Episode 155: 150.0\n",
      "Episode 156: 120.0\n",
      "Episode 157: 260.0\n",
      "Episode 158: 70.0\n",
      "Episode 159: 110.0\n",
      "Episode 160: 75.0\n",
      "Episode 161: 155.0\n",
      "Episode 162: 170.0\n",
      "Episode 163: 90.0\n",
      "Episode 164: 240.0\n",
      "Episode 165: 90.0\n",
      "Episode 166: 95.0\n",
      "Episode 167: 245.0\n",
      "Episode 168: 55.0\n",
      "Episode 169: 120.0\n",
      "Episode 170: 150.0\n",
      "Best score of last 100: 415.0, running reward: 152.50 at episode 171, frame count 90000\n",
      "Episode 171: 160.0\n",
      "Episode 172: 110.0\n",
      "Episode 173: 65.0\n",
      "Episode 174: 210.0\n",
      "Episode 175: 90.0\n",
      "Episode 176: 245.0\n",
      "Episode 177: 170.0\n",
      "Episode 178: 35.0\n",
      "Episode 179: 55.0\n",
      "Episode 180: 30.0\n",
      "Episode 181: 155.0\n",
      "Episode 182: 60.0\n",
      "Episode 183: 120.0\n",
      "Episode 184: 135.0\n",
      "Episode 185: 85.0\n",
      "Episode 186: 155.0\n",
      "Episode 187: 120.0\n",
      "Episode 188: 120.0\n",
      "Episode 189: 130.0\n",
      "Episode 190: 50.0\n",
      "Episode 191: 35.0\n",
      "Best score of last 100: 415.0, running reward: 144.80 at episode 192, frame count 100000\n",
      "Episode 192: 185.0\n",
      "Episode 193: 155.0\n",
      "Episode 194: 145.0\n",
      "Episode 195: 50.0\n",
      "Episode 196: 55.0\n",
      "Episode 197: 225.0\n",
      "Episode 198: 60.0\n",
      "Episode 199: 60.0\n",
      "Episode 200: 195.0\n",
      "Episode 201: 55.0\n",
      "Episode 202: 130.0\n",
      "Episode 203: 155.0\n",
      "Episode 204: 110.0\n",
      "Episode 205: 65.0\n",
      "Episode 206: 120.0\n",
      "Episode 207: 105.0\n",
      "Episode 208: 150.0\n",
      "Episode 209: 155.0\n",
      "Episode 210: 215.0\n",
      "Episode 211: 60.0\n",
      "Best score of last 100: 415.0, running reward: 135.20 at episode 212, frame count 110000\n",
      "Episode 212: 170.0\n",
      "Episode 213: 80.0\n",
      "Episode 214: 225.0\n",
      "Episode 215: 125.0\n",
      "Episode 216: 55.0\n",
      "Episode 217: 5.0\n",
      "Episode 218: 125.0\n",
      "Episode 219: 50.0\n",
      "Episode 220: 215.0\n",
      "Episode 221: 50.0\n",
      "Episode 222: 35.0\n",
      "Episode 223: 120.0\n",
      "Episode 224: 415.0\n",
      "Episode 225: 105.0\n",
      "Episode 226: 230.0\n",
      "Episode 227: 180.0\n",
      "Episode 228: 90.0\n",
      "Episode 229: 105.0\n",
      "Episode 230: 155.0\n",
      "Best score of last 100: 415.0, running reward: 131.70 at episode 231, frame count 120000\n",
      "Episode 231: 195.0\n",
      "Episode 232: 155.0\n",
      "Episode 233: 25.0\n",
      "Episode 234: 120.0\n",
      "Episode 235: 205.0\n",
      "Episode 236: 210.0\n",
      "Episode 237: 15.0\n",
      "Episode 238: 45.0\n",
      "Episode 239: 320.0\n",
      "Episode 240: 140.0\n",
      "Episode 241: 45.0\n",
      "Episode 242: 210.0\n",
      "Episode 243: 135.0\n",
      "Episode 244: 335.0\n",
      "Episode 245: 185.0\n",
      "Episode 246: 215.0\n",
      "Episode 247: 185.0\n",
      "Episode 248: 155.0\n",
      "Episode 249: 185.0\n",
      "Episode 250: 110.0\n",
      "Best score of last 100: 415.0, running reward: 134.10 at episode 251, frame count 130000\n",
      "Episode 251: 105.0\n",
      "Episode 252: 175.0\n",
      "Episode 253: 30.0\n",
      "Episode 254: 65.0\n",
      "Episode 255: 120.0\n",
      "Episode 256: 20.0\n",
      "Episode 257: 150.0\n",
      "Episode 258: 80.0\n",
      "Episode 259: 210.0\n",
      "Episode 260: 190.0\n",
      "Episode 261: 60.0\n",
      "Episode 262: 245.0\n",
      "Episode 263: 395.0\n",
      "Episode 264: 135.0\n",
      "Episode 265: 125.0\n",
      "Episode 266: 125.0\n",
      "Episode 267: 100.0\n",
      "Episode 268: 30.0\n",
      "Episode 269: 75.0\n",
      "Episode 270: 135.0\n",
      "Best score of last 100: 415.0, running reward: 130.80 at episode 271, frame count 140000\n",
      "Episode 271: 195.0\n",
      "Episode 272: 135.0\n",
      "Episode 273: 215.0\n",
      "Episode 274: 160.0\n",
      "Episode 275: 45.0\n",
      "Episode 276: 65.0\n",
      "Episode 277: 135.0\n",
      "Episode 278: 170.0\n",
      "Episode 279: 65.0\n",
      "Episode 280: 65.0\n",
      "Episode 281: 45.0\n",
      "Episode 282: 90.0\n",
      "Episode 283: 155.0\n",
      "Episode 284: 110.0\n",
      "Episode 285: 65.0\n",
      "Episode 286: 335.0\n",
      "Episode 287: 150.0\n",
      "Episode 288: 65.0\n",
      "Episode 289: 45.0\n",
      "Episode 290: 75.0\n",
      "Episode 291: 35.0\n",
      "Episode 292: 210.0\n",
      "Best score of last 100: 415.0, running reward: 131.90 at episode 293, frame count 150000\n",
      "Episode 293: 35.0\n",
      "Episode 294: 35.0\n",
      "Episode 295: 90.0\n",
      "Episode 296: 180.0\n",
      "Episode 297: 130.0\n",
      "Episode 298: 80.0\n",
      "Episode 299: 100.0\n",
      "Episode 300: 105.0\n",
      "Episode 301: 30.0\n",
      "Episode 302: 75.0\n",
      "Episode 303: 120.0\n",
      "Episode 304: 70.0\n",
      "Episode 305: 100.0\n",
      "Episode 306: 165.0\n",
      "Episode 307: 200.0\n",
      "Episode 308: 40.0\n",
      "Episode 309: 40.0\n",
      "Episode 310: 320.0\n",
      "Episode 311: 195.0\n",
      "Episode 312: 180.0\n",
      "Episode 313: 210.0\n",
      "Best score of last 100: 415.0, running reward: 131.75 at episode 314, frame count 160000\n",
      "Episode 314: 215.0\n",
      "Episode 315: 240.0\n",
      "Episode 316: 75.0\n",
      "Episode 317: 45.0\n",
      "Episode 318: 125.0\n",
      "Episode 319: 30.0\n",
      "Episode 320: 230.0\n",
      "Episode 321: 110.0\n",
      "Episode 322: 300.0\n",
      "Episode 323: 105.0\n",
      "Episode 324: 295.0\n",
      "Episode 325: 15.0\n",
      "Episode 326: 65.0\n",
      "Episode 327: 80.0\n",
      "Episode 328: 145.0\n",
      "Episode 329: 110.0\n",
      "Episode 330: 155.0\n",
      "Episode 331: 210.0\n",
      "Episode 332: 90.0\n",
      "Episode 333: 135.0\n",
      "Best score of last 100: 395.0, running reward: 132.90 at episode 334, frame count 170000\n",
      "Episode 334: 165.0\n",
      "Episode 335: 45.0\n",
      "Episode 336: 20.0\n",
      "Episode 337: 180.0\n",
      "Episode 338: 260.0\n",
      "Episode 339: 125.0\n",
      "Episode 340: 65.0\n",
      "Episode 341: 150.0\n",
      "Episode 342: 10.0\n",
      "Episode 343: 215.0\n",
      "Episode 344: 65.0\n",
      "Episode 345: 155.0\n",
      "Episode 346: 145.0\n",
      "Episode 347: 210.0\n",
      "Episode 348: 180.0\n",
      "Episode 349: 195.0\n",
      "Episode 350: 425.0\n",
      "Episode 351: 35.0\n",
      "Episode 352: 210.0\n",
      "Best score of last 100: 425.0, running reward: 130.50 at episode 353, frame count 180000\n",
      "Episode 353: 100.0\n",
      "Episode 354: 110.0\n",
      "Episode 355: 155.0\n",
      "Episode 356: 300.0\n",
      "Episode 357: 70.0\n",
      "Episode 358: 90.0\n",
      "Episode 359: 90.0\n",
      "Episode 360: 45.0\n",
      "Episode 361: 60.0\n",
      "Episode 362: 155.0\n",
      "Episode 363: 195.0\n",
      "Episode 364: 65.0\n",
      "Episode 365: 180.0\n",
      "Episode 366: 355.0\n",
      "Episode 367: 505.0\n",
      "Episode 368: 55.0\n",
      "Episode 369: 155.0\n",
      "Episode 370: 555.0\n",
      "Best score of last 100: 555.0, running reward: 140.00 at episode 371, frame count 190000\n",
      "Episode 371: 95.0\n",
      "Episode 372: 260.0\n",
      "Episode 373: 270.0\n",
      "Episode 374: 165.0\n",
      "Episode 375: 375.0\n",
      "Episode 376: 100.0\n",
      "Episode 377: 40.0\n",
      "Episode 378: 140.0\n",
      "Episode 379: 180.0\n",
      "Episode 380: 125.0\n",
      "Episode 381: 155.0\n",
      "Episode 382: 45.0\n",
      "Episode 383: 210.0\n",
      "Episode 384: 110.0\n",
      "Episode 385: 70.0\n",
      "Episode 386: 410.0\n",
      "Episode 387: 210.0\n",
      "Episode 388: 50.0\n",
      "Best score of last 100: 555.0, running reward: 147.45 at episode 389, frame count 200000\n",
      "Episode 389: 85.0\n",
      "Episode 390: 120.0\n",
      "Episode 391: 60.0\n",
      "Episode 392: 135.0\n",
      "Episode 393: 195.0\n",
      "Episode 394: 45.0\n",
      "Episode 395: 170.0\n",
      "Episode 396: 120.0\n",
      "Episode 397: 60.0\n",
      "Episode 398: 265.0\n",
      "Episode 399: 130.0\n",
      "Episode 400: 105.0\n",
      "Episode 401: 155.0\n",
      "Episode 402: 45.0\n",
      "Episode 403: 55.0\n",
      "Episode 404: 180.0\n",
      "Episode 405: 55.0\n",
      "Episode 406: 185.0\n",
      "Episode 407: 125.0\n",
      "Episode 408: 80.0\n",
      "Best score of last 100: 555.0, running reward: 151.95 at episode 409, frame count 210000\n",
      "Episode 409: 310.0\n",
      "Episode 410: 30.0\n",
      "Episode 411: 205.0\n",
      "Episode 412: 130.0\n",
      "Episode 413: 85.0\n",
      "Episode 414: 130.0\n",
      "Episode 415: 185.0\n",
      "Episode 416: 225.0\n",
      "Episode 417: 150.0\n",
      "Episode 418: 110.0\n",
      "Episode 419: 80.0\n",
      "Episode 420: 80.0\n",
      "Episode 421: 195.0\n",
      "Episode 422: 35.0\n",
      "Episode 423: 210.0\n",
      "Episode 424: 135.0\n",
      "Episode 425: 395.0\n",
      "Episode 426: 155.0\n",
      "Episode 427: 100.0\n",
      "Episode 428: 40.0\n",
      "Episode 429: 50.0\n",
      "Episode 430: 155.0\n",
      "Best score of last 100: 555.0, running reward: 151.00 at episode 431, frame count 220000\n",
      "Episode 431: 55.0\n",
      "Episode 432: 35.0\n",
      "Episode 433: 135.0\n",
      "Episode 434: 30.0\n",
      "Episode 435: 20.0\n",
      "Episode 436: 60.0\n",
      "Episode 437: 30.0\n",
      "Episode 438: 55.0\n",
      "Episode 439: 110.0\n",
      "Episode 440: 110.0\n",
      "Episode 441: 80.0\n",
      "Episode 442: 45.0\n",
      "Episode 443: 125.0\n",
      "Episode 444: 105.0\n",
      "Episode 445: 105.0\n",
      "Episode 446: 120.0\n",
      "Episode 447: 105.0\n",
      "Episode 448: 215.0\n",
      "Episode 449: 50.0\n",
      "Episode 450: 360.0\n",
      "Episode 451: 135.0\n",
      "Episode 452: 535.0\n",
      "Episode 453: 35.0\n",
      "Best score of last 100: 555.0, running reward: 143.65 at episode 454, frame count 230000\n",
      "Episode 454: 95.0\n",
      "Episode 455: 80.0\n",
      "Episode 456: 30.0\n",
      "Episode 457: 120.0\n",
      "Episode 458: 290.0\n",
      "Episode 459: 65.0\n",
      "Episode 460: 50.0\n",
      "Episode 461: 145.0\n",
      "Episode 462: 30.0\n",
      "Episode 463: 155.0\n",
      "Episode 464: 195.0\n",
      "Episode 465: 130.0\n",
      "Episode 466: 125.0\n",
      "Episode 467: 135.0\n",
      "Episode 468: 65.0\n",
      "Episode 469: 150.0\n",
      "Episode 470: 130.0\n",
      "Episode 471: 455.0\n",
      "Episode 472: 50.0\n",
      "Episode 473: 25.0\n",
      "Best score of last 100: 535.0, running reward: 131.20 at episode 474, frame count 240000\n",
      "Episode 474: 40.0\n",
      "Episode 475: 110.0\n",
      "Episode 476: 210.0\n",
      "Episode 477: 155.0\n",
      "Episode 478: 215.0\n",
      "Episode 479: 140.0\n",
      "Episode 480: 110.0\n",
      "Episode 481: 55.0\n",
      "Episode 482: 35.0\n",
      "Episode 483: 120.0\n",
      "Episode 484: 120.0\n",
      "Episode 485: 200.0\n",
      "Episode 486: 125.0\n",
      "Episode 487: 55.0\n",
      "Episode 488: 400.0\n",
      "Episode 489: 155.0\n",
      "Episode 490: 155.0\n",
      "Episode 491: 155.0\n",
      "Episode 492: 180.0\n",
      "Best score of last 100: 535.0, running reward: 130.70 at episode 493, frame count 250000\n",
      "Episode 493: 160.0\n",
      "Episode 494: 205.0\n",
      "Episode 495: 165.0\n",
      "Episode 496: 135.0\n",
      "Episode 497: 215.0\n",
      "Episode 498: 15.0\n",
      "Episode 499: 135.0\n",
      "Episode 500: 105.0\n",
      "Episode 501: 100.0\n",
      "Episode 502: 65.0\n",
      "Episode 503: 330.0\n",
      "Episode 504: 180.0\n",
      "Episode 505: 45.0\n",
      "Episode 506: 210.0\n",
      "Episode 507: 400.0\n",
      "Episode 508: 35.0\n",
      "Episode 509: 205.0\n",
      "Episode 510: 140.0\n",
      "Best score of last 100: 535.0, running reward: 136.05 at episode 511, frame count 260000\n",
      "Episode 511: 145.0\n",
      "Episode 512: 285.0\n",
      "Episode 513: 155.0\n",
      "Episode 514: 105.0\n",
      "Episode 515: 45.0\n",
      "Episode 516: 530.0\n",
      "Episode 517: 30.0\n",
      "Episode 518: 90.0\n",
      "Episode 519: 80.0\n",
      "Episode 520: 120.0\n",
      "Episode 521: 105.0\n",
      "Episode 522: 140.0\n",
      "Episode 523: 120.0\n",
      "Episode 524: 110.0\n",
      "Episode 525: 230.0\n",
      "Episode 526: 125.0\n",
      "Episode 527: 250.0\n",
      "Episode 528: 155.0\n",
      "Episode 529: 45.0\n",
      "Episode 530: 80.0\n",
      "Best score of last 100: 535.0, running reward: 137.00 at episode 531, frame count 270000\n",
      "Episode 531: 350.0\n",
      "Episode 532: 195.0\n",
      "Episode 533: 105.0\n",
      "Episode 534: 95.0\n",
      "Episode 535: 90.0\n",
      "Episode 536: 285.0\n",
      "Episode 537: 185.0\n",
      "Episode 538: 120.0\n",
      "Episode 539: 135.0\n",
      "Episode 540: 140.0\n",
      "Episode 541: 320.0\n",
      "Episode 542: 265.0\n",
      "Episode 543: 185.0\n",
      "Episode 544: 170.0\n",
      "Episode 545: 225.0\n",
      "Episode 546: 175.0\n",
      "Best score of last 100: 535.0, running reward: 155.20 at episode 547, frame count 280000\n",
      "Episode 547: 80.0\n",
      "Episode 548: 50.0\n",
      "Episode 549: 75.0\n",
      "Episode 550: 160.0\n",
      "Episode 551: 55.0\n",
      "Episode 552: 155.0\n",
      "Episode 553: 245.0\n",
      "Episode 554: 120.0\n",
      "Episode 555: 110.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[208], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(epsilon, epsilon_min)  \u001b[38;5;66;03m# Begränsar epsilon till minimumvärdet.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Utför åtgärden i miljön och observerar resultatet.\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m state_next, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     31\u001b[0m state_next \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(state_next)  \u001b[38;5;66;03m# Konverterar nästa tillstånd till NumPy-array.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# Lägg till belöningen från detta steg till episodens totala belöning.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\record_video.py:166\u001b[0m, in \u001b[0;36mRecordVideo.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment using action, recording observations if :attr:`self.recording`.\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     (\n\u001b[0;32m    161\u001b[0m         observations,\n\u001b[0;32m    162\u001b[0m         rewards,\n\u001b[0;32m    163\u001b[0m         terminateds,\n\u001b[0;32m    164\u001b[0m         truncateds,\n\u001b[0;32m    165\u001b[0m         infos,\n\u001b[1;32m--> 166\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminated \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncated):\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;66;03m# increment steps and episodes\u001b[39;00m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\frame_stack.py:179\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and information from the environment\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(\u001b[38;5;28;01mNone\u001b[39;00m), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\Ars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\atari_preprocessing.py:138\u001b[0m, in \u001b[0;36mAtariPreprocessing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    135\u001b[0m total_reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_skip):\n\u001b[1;32m--> 138\u001b[0m     _, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_over \u001b[38;5;241m=\u001b[39m terminated\n",
      "File \u001b[1;32mc:\\Users\\Ars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ale_py\\env.py:304\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    301\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    302\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n\u001b[1;32m--> 304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs(), reward, is_terminal, is_truncated, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ale_py\\env.py:331\u001b[0m, in \u001b[0;36mAtariEnv._get_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\n\u001b[0;32m    328\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized observation type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mram\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrayscale\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    329\u001b[0m         )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_info\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AtariEnvStepMetadata:\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlives\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mlives(),\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_frame_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgetEpisodeFrameNumber(),\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgetFrameNumber(),\n\u001b[0;32m    336\u001b[0m     }\n\u001b[0;32m    338\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_keys_to_action\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], ale_py\u001b[38;5;241m.\u001b[39mAction]:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:  # En oändlig loop som tränar modellen tills ett avslutande villkor nås.\n",
    "    observation, _ = env.reset()  # Nollställer miljön och hämtar den initiala observationen.\n",
    "    state = np.array(observation)  # Konverterar observationen till en NumPy-array.\n",
    "    episode_reward = 0  # Initierar episodens totala belöning.\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):  # Begränsar antalet steg per episod.\n",
    "        frame_count += 1  # Ökar den totala bildramräknaren (används för tidsstyrda åtgärder).\n",
    "\n",
    "        # Epsilon-greedy policy: Välj antingen en slumpmässig åtgärd eller den bästa åtgärden från modellen.\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Utforskar genom att ta en slumpmässig åtgärd.\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Uppdatera epsilon-greedy steget\n",
    "            state_processed = preprocess_input(state) #------------------------------------------------\n",
    "            state_tensor = keras.ops.convert_to_tensor(state_processed)\n",
    "            state_tensor = keras.ops.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # # Exploaterar genom att använda modellen för att välja den bästa åtgärden.\n",
    "            # state_tensor = keras.ops.convert_to_tensor(state)  # Konverterar tillståndet till en tensor.\n",
    "            # state_tensor = keras.ops.expand_dims(state_tensor, 0)  # Lägger till batch-dimension.\n",
    "            # action_probs = model(state_tensor, training=False)  # Förutspår Q-värden utan att aktivera träningsläge.\n",
    "            # action = keras.ops.argmax(action_probs[0]).numpy()  # Väljer åtgärden med högst Q-värde.\n",
    "\n",
    "        # Minskar epsilon linjärt för att gradvis minska sannolikheten för slumpmässiga åtgärder.\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)  # Begränsar epsilon till minimumvärdet.\n",
    "\n",
    "        # Utför åtgärden i miljön och observerar resultatet.\n",
    "        state_next, reward, done, _, _ = env.step(action)  \n",
    "        state_next = np.array(state_next)  # Konverterar nästa tillstånd till NumPy-array.\n",
    "\n",
    "        episode_reward += reward  # Lägg till belöningen från detta steg till episodens totala belöning.\n",
    "\n",
    "        # Spara övergången i replay-buffer.\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "\n",
    "        # Gå vidare till nästa tillstånd.\n",
    "        state = state_next\n",
    "        \n",
    "        # Träna modellen efter var fjärde åtgärd och om replay-buffer är tillräckligt stor.\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Väljer slumpmässigt ett urval av tidigare övergångar för att skapa träningsdata.\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Extraherar de valda övergångarna från replay-buffer.\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = keras.ops.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            state_next_sample_processed = np.array([preprocess_input(s) for s in state_next_sample])\n",
    "            future_rewards = model_target.predict(state_next_sample_processed, verbose=0)\n",
    "\n",
    "            #future_rewards = model_target.predict(state_next_sample, verbose=0)---------------------------------------------------------\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # Uppdaterade Q-värden: belöning + gamma * max framtida belöning.\n",
    "            updated_q_values = rewards_sample + gamma * keras.ops.amax(future_rewards, axis=1)\n",
    "\n",
    "            # Sätter Q-värdet till -1 om tillståndet är terminalt.\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Skapar en mask för att beräkna förlust endast för den utförda åtgärden.\n",
    "            masks = keras.ops.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Förutspår Q-värden för det aktuella tillståndet.\n",
    "                #q_values = model(state_sample)------------------------------------------------------------------------------------------------\n",
    "            \n",
    "            \n",
    "\n",
    "                state_sample_processed = np.array([preprocess_input(s) for s in state_sample])\n",
    "                q_values = model(state_sample_processed)\n",
    "\n",
    "\n",
    "                # Extraherar Q-värdet för den valda åtgärden.\n",
    "                q_action = keras.ops.sum(keras.ops.multiply(q_values, masks), axis=1)\n",
    "\n",
    "                # Beräknar förlusten mellan de förväntade och de faktiska Q-värdena.\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Utför backpropagation för att uppdatera modellens viktparametrar.\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Uppdaterar target-modellen efter ett fast antal frames.\n",
    "        if frame_count % update_target_network == 0:\n",
    "            model_target.set_weights(model.get_weights())  # Kopierar vikterna från träningsmodellen.\n",
    "            print(f\"Best score of last 100: {np.max(episode_reward_history)}, \n",
    "                  running reward: {running_reward:.2f} at episode {episode_count}, frame count {frame_count}\")\n",
    "            model.save(f\"models/breakout_qmodel_{episode_count}.keras\")  # Sparar modellen efter varje uppdatering.\n",
    "\n",
    "        # Begränsar replay-buffer till maxlängd genom att ta bort gamla övergångar.\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        # Om episoden är klar, avsluta loopen för denna episod.\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Uppdaterar löpande belöning baserat på de senaste 100 episoderna.\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    # Ökar episodräknaren.\n",
    "    episode_count += 1\n",
    "    print(f\"Episode {episode_count-1}: {episode_reward}\")\n",
    "\n",
    "    # Kontrollera om problemet anses löst baserat på en belöningströskel.\n",
    "    if running_reward > 800:\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n",
    "\n",
    "    # Avbryter om max episoder eller max frames har nåtts.\n",
    "    if max_episodes > 0 and episode_count >= max_episodes:\n",
    "        print(\"Stopped at episode {}!\".format(episode_count))\n",
    "        break\n",
    "    if max_frames <= frame_count:\n",
    "        print(f\"Stopped at frame {frame_count}!\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
